{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "739a69a5",
   "metadata": {},
   "source": [
    "# üéØ BillingRag - Databricks Setup Notebook\n",
    "\n",
    "**Run this notebook in Databricks to set up everything you need.**\n",
    "\n",
    "## What this does:\n",
    "1. Creates the schema\n",
    "2. Creates the 2 tables (rag_documents, hvs_data)\n",
    "3. Inserts sample data for testing\n",
    "4. Verifies everything works\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a5bab",
   "metadata": {},
   "source": [
    "## üìã Step 1: Configuration\n",
    "\n",
    "Change these values to match your setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea594cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - CHANGE THESE!\n",
    "CATALOG = \"main\"  # or your catalog name\n",
    "SCHEMA = \"billingrag_tatsat\"  # your schema name\n",
    "\n",
    "# Full table names\n",
    "RAG_TABLE = f\"{CATALOG}.{SCHEMA}.rag_documents\"\n",
    "HVS_TABLE = f\"{CATALOG}.{SCHEMA}.teleco_data\"\n",
    "\n",
    "print(f\"üìç Catalog: {CATALOG}\")\n",
    "print(f\"üìç Schema: {SCHEMA}\")\n",
    "print(f\"üìç RAG Table: {RAG_TABLE}\")\n",
    "print(f\"üìç HVS Table: {HVS_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dffc16",
   "metadata": {},
   "source": [
    "## üìã Step 2: Create Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a1931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create schema if it doesn't exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "print(f\"‚úÖ Schema {CATALOG}.{SCHEMA} created/exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b66ce",
   "metadata": {},
   "source": [
    "## üìã Step 3: Create RAG Documents Table\n",
    "\n",
    "This table stores your knowledge base:\n",
    "- `text` - Document chunks\n",
    "- `embedding` - Vector embeddings (384 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b0c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG Documents table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {RAG_TABLE} (\n",
    "    id STRING NOT NULL COMMENT 'Unique chunk ID (UUID)',\n",
    "    filename STRING COMMENT 'Original source filename',\n",
    "    text STRING COMMENT 'Chunk text content',\n",
    "    embedding ARRAY<FLOAT> COMMENT 'Vector embedding (384 dimensions)',\n",
    "    chunk_index INT COMMENT 'Position of chunk in original document',\n",
    "    upload_date TIMESTAMP COMMENT 'When this chunk was uploaded',\n",
    "    metadata MAP<STRING, STRING> COMMENT 'Additional metadata like source, tags'\n",
    ") \n",
    "USING DELTA\n",
    "COMMENT 'RAG knowledge base - document chunks with vector embeddings for semantic search'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Table {RAG_TABLE} created/exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990d198",
   "metadata": {},
   "source": [
    "## üìã Step 4: Create HVS Data Table\n",
    "\n",
    "This table stores billing data for ML predictions:\n",
    "- Enterprise/subaccount billing rates\n",
    "- Used to train the prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e122b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HVS Data table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {HVS_TABLE} (\n",
    "    id STRING NOT NULL COMMENT 'Unique record ID (UUID)',\n",
    "    enterprise_id STRING COMMENT 'Enterprise/company identifier',\n",
    "    subaccount STRING COMMENT 'Subaccount identifier within enterprise',\n",
    "    device_type STRING COMMENT 'Device type: ROUTER, SWITCH, FIREWALL, etc',\n",
    "    service_type STRING COMMENT 'Service type: INTERNET, VPN, MPLS, SD-WAN',\n",
    "    monthly_rate DECIMAL(10,2) COMMENT 'Monthly billing rate in dollars',\n",
    "    bandwidth_mbps INT COMMENT 'Provisioned bandwidth in Mbps',\n",
    "    contract_start DATE COMMENT 'Contract start date',\n",
    "    contract_end DATE COMMENT 'Contract end date',\n",
    "    status STRING COMMENT 'Status: ACTIVE, INACTIVE, SUSPENDED',\n",
    "    upload_date TIMESTAMP COMMENT 'When this record was uploaded to system',\n",
    "    data_month STRING COMMENT 'Source data month in YYYY-MM format'\n",
    ")\n",
    "USING DELTA\n",
    "COMMENT 'HVS billing data for ML rate predictions - retains 3-4 months of history'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Table {HVS_TABLE} created/exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ff9ef",
   "metadata": {},
   "source": [
    "## üìã Step 5: Verify Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all tables in schema\n",
    "tables = spark.sql(f\"SHOW TABLES IN {CATALOG}.{SCHEMA}\")\n",
    "display(tables)\n",
    "\n",
    "print(\"\\nüìä Table Details:\")\n",
    "for table in [RAG_TABLE, HVS_TABLE]:\n",
    "    count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {table}\").first()['cnt']\n",
    "    print(f\"   {table}: {count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a18f0ac",
   "metadata": {},
   "source": [
    "## üìã Step 6: Insert Sample Data (Optional)\n",
    "\n",
    "Run this to add test data for development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert sample RAG document\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "sample_doc_id = str(uuid.uuid4())\n",
    "sample_embedding = [0.1] * 384  # Mock 384-dimension embedding\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {RAG_TABLE}\n",
    "VALUES (\n",
    "    '{sample_doc_id}',\n",
    "    'sample_billing_policy.txt',\n",
    "    'This is a sample billing policy document. Enterprise billing rates are calculated based on bandwidth consumption, service type, and contract terms. Standard rates apply for INTERNET services while premium rates apply for MPLS and SD-WAN services.',\n",
    "    array({','.join(map(str, sample_embedding))}),\n",
    "    0,\n",
    "    current_timestamp(),\n",
    "    map('source', 'sample', 'type', 'policy')\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Sample document inserted: {sample_doc_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b111bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert sample HVS data\n",
    "import uuid\n",
    "\n",
    "sample_hvs_data = [\n",
    "    ('ENT001', 'SUB001', 'ROUTER', 'INTERNET', 250.00, 100, '2025-01-01', '2026-01-01', 'ACTIVE'),\n",
    "    ('ENT001', 'SUB002', 'SWITCH', 'VPN', 175.50, 50, '2025-02-01', '2026-02-01', 'ACTIVE'),\n",
    "    ('ENT001', 'SUB003', 'FIREWALL', 'INTERNET', 320.00, 200, '2025-01-15', '2026-01-15', 'ACTIVE'),\n",
    "    ('ENT002', 'SUB001', 'ROUTER', 'MPLS', 500.00, 500, '2024-06-01', '2025-06-01', 'ACTIVE'),\n",
    "    ('ENT002', 'SUB002', 'ROUTER', 'SD-WAN', 450.00, 300, '2024-09-01', '2025-09-01', 'ACTIVE'),\n",
    "    ('ENT003', 'SUB001', 'SWITCH', 'INTERNET', 125.00, 50, '2025-03-01', '2026-03-01', 'ACTIVE'),\n",
    "    ('ENT003', 'SUB002', 'ROUTER', 'VPN', 200.00, 100, '2025-01-01', '2026-01-01', 'ACTIVE'),\n",
    "    ('ENT004', 'SUB001', 'FIREWALL', 'MPLS', 600.00, 1000, '2024-12-01', '2025-12-01', 'ACTIVE'),\n",
    "    ('ENT004', 'SUB002', 'ROUTER', 'INTERNET', 180.00, 100, '2025-02-01', '2026-02-01', 'ACTIVE'),\n",
    "    ('ENT005', 'SUB001', 'ROUTER', 'SD-WAN', 350.00, 200, '2025-01-01', '2026-01-01', 'ACTIVE'),\n",
    "]\n",
    "\n",
    "for ent, sub, dev, svc, rate, bw, start, end, status in sample_hvs_data:\n",
    "    record_id = str(uuid.uuid4())\n",
    "    spark.sql(f\"\"\"\n",
    "    INSERT INTO {HVS_TABLE}\n",
    "    VALUES (\n",
    "        '{record_id}',\n",
    "        '{ent}',\n",
    "        '{sub}',\n",
    "        '{dev}',\n",
    "        '{svc}',\n",
    "        {rate},\n",
    "        {bw},\n",
    "        '{start}',\n",
    "        '{end}',\n",
    "        '{status}',\n",
    "        current_timestamp(),\n",
    "        '2025-01'\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "print(f\"‚úÖ Inserted {len(sample_hvs_data)} sample HVS records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd71ccc",
   "metadata": {},
   "source": [
    "## üìã Step 7: Query Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e713cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check RAG documents\n",
    "print(\"üìÑ RAG Documents:\")\n",
    "display(spark.sql(f\"SELECT id, filename, LEFT(text, 100) as text_preview, upload_date FROM {RAG_TABLE} LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996bc271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check HVS data\n",
    "print(\"üí∞ HVS Data:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT enterprise_id, subaccount, device_type, service_type, monthly_rate, bandwidth_mbps, status\n",
    "    FROM {HVS_TABLE}\n",
    "    ORDER BY enterprise_id, subaccount\n",
    "    LIMIT 20\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HVS Summary Statistics\n",
    "print(\"üìä HVS Summary:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(DISTINCT enterprise_id) as unique_enterprises,\n",
    "        COUNT(DISTINCT subaccount) as unique_subaccounts,\n",
    "        AVG(monthly_rate) as avg_rate,\n",
    "        MIN(monthly_rate) as min_rate,\n",
    "        MAX(monthly_rate) as max_rate,\n",
    "        AVG(bandwidth_mbps) as avg_bandwidth\n",
    "    FROM {HVS_TABLE}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b5abd",
   "metadata": {},
   "source": [
    "## ‚úÖ Setup Complete!\n",
    "\n",
    "You now have:\n",
    "1. **Schema**: `main.billingrag_tatsat`\n",
    "2. **RAG Table**: `rag_documents` - for knowledge base\n",
    "3. **HVS Table**: `hvs_data` - for ML predictions\n",
    "4. **Sample Data**: Ready for testing\n",
    "\n",
    "### Next Steps:\n",
    "1. Copy your Databricks credentials\n",
    "2. Update `.env` file in your local project\n",
    "3. Run `python run.py test` to verify connection\n",
    "4. Run `python run.py serve` to start the app\n",
    "\n",
    "### Connection Details You Need:\n",
    "- **Host**: Your workspace URL (e.g., `https://xxx.cloud.databricks.com`)\n",
    "- **Token**: Generate from User Settings ‚Üí Developer ‚Üí Access Tokens\n",
    "- **HTTP Path**: From SQL Warehouse ‚Üí Connection Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64957afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print connection info (DON'T SHARE TOKEN!)\n",
    "import os\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"üìã YOUR CONFIGURATION (for .env file)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\"\"\n",
    "# Copy these to your .env file:\n",
    "\n",
    "DATABRICKS_HOST={spark.conf.get('spark.databricks.workspaceUrl', 'YOUR_WORKSPACE_URL')}\n",
    "DATABRICKS_TOKEN=dapi_your_token_here\n",
    "DATABRICKS_HTTP_PATH=/sql/1.0/warehouses/YOUR_WAREHOUSE_ID\n",
    "\n",
    "DATABRICKS_CATALOG={CATALOG}\n",
    "DATABRICKS_SCHEMA={SCHEMA}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
