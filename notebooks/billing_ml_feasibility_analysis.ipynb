{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78c823e",
   "metadata": {},
   "source": [
    "# üìä Billing Data ML Feasibility Analysis\n",
    "\n",
    "## Objective\n",
    "Analyze the feasibility of training ML models on monthly billing data for:\n",
    "1. **Knowledge-based Q&A** - RAG system to answer questions about billing data\n",
    "2. **Trend Prediction** - Forecast future billing trends using ARIMA/Prophet\n",
    "3. **Anomaly Detection** - Identify unusual billing patterns\n",
    "\n",
    "## Data Source\n",
    "Monthly Excel billing exports (e.g., `Export of Trending Dashboard Report_TAB5_dec 1.xlsx`)\n",
    "\n",
    "## Key Questions\n",
    "- Is the data sufficient for ML training?\n",
    "- What's the best model storage strategy?\n",
    "- Can we use Replit for deployment?\n",
    "- How to set up monthly retraining?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd8eb2c",
   "metadata": {},
   "source": [
    "## Section 1: Load and Explore Billing Data\n",
    "\n",
    "First, let's load the Excel file and understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'pandas', 'numpy', 'openpyxl', 'scikit-learn', \n",
    "    'matplotlib', 'seaborn', 'joblib'\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
    "\n",
    "print(\"‚úÖ Required packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360fe859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Load the Excel file\n",
    "excel_path = Path(\"../Export of Trending Dashboard Report_TAB5_dec 1.xlsx\")\n",
    "\n",
    "# Read all sheets to understand structure\n",
    "xls = pd.ExcelFile(excel_path)\n",
    "print(f\"üìÅ File: {excel_path.name}\")\n",
    "print(f\"üìã Available sheets: {xls.sheet_names}\")\n",
    "\n",
    "# Load the first sheet (or main data)\n",
    "df_raw = pd.read_excel(excel_path, header=None)\n",
    "print(f\"\\nüìä Raw data shape: {df_raw.shape}\")\n",
    "print(f\"\\nüîç First 25 rows (raw view):\")\n",
    "df_raw.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef612b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify header row and clean data\n",
    "# Look for the row with actual column headers\n",
    "\n",
    "def find_header_row(df, max_rows=20):\n",
    "    \"\"\"Find the row that looks like column headers.\"\"\"\n",
    "    for i in range(min(max_rows, len(df))):\n",
    "        row = df.iloc[i]\n",
    "        # Count non-null values and check if they look like headers\n",
    "        non_null = row.dropna()\n",
    "        if len(non_null) >= 3:\n",
    "            # Check if values are strings (likely headers)\n",
    "            if all(isinstance(v, str) for v in non_null):\n",
    "                return i\n",
    "    return 0\n",
    "\n",
    "header_row = find_header_row(df_raw)\n",
    "print(f\"üéØ Detected header row: {header_row}\")\n",
    "\n",
    "# Reload with proper header\n",
    "df = pd.read_excel(excel_path, header=header_row)\n",
    "\n",
    "# Clean column names\n",
    "df.columns = [str(col).strip().replace('\\n', ' ') for col in df.columns]\n",
    "\n",
    "print(f\"\\nüìã Cleaned columns ({len(df.columns)}):\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"  {i+1}. {col}\")\n",
    "\n",
    "print(f\"\\nüìä Data shape after cleaning: {df.shape}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ddfb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data types and summary statistics\n",
    "print(\"üìä DATA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal Records: {len(df)}\")\n",
    "print(f\"Total Columns: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\nüìã Data Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\nüìà Numeric Columns Statistics:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if numeric_cols:\n",
    "    print(df[numeric_cols].describe())\n",
    "else:\n",
    "    print(\"No numeric columns detected - data may need type conversion\")\n",
    "\n",
    "print(\"\\n‚ùì Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Missing': missing, 'Percent': missing_pct})\n",
    "print(missing_df[missing_df['Missing'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f35a46",
   "metadata": {},
   "source": [
    "## Section 2: Data Preprocessing and Schema Detection\n",
    "\n",
    "Implementing schema detection from the STATISTICAL_MODELS_IMPLEMENTATION_PLAN.md to auto-identify:\n",
    "- Timestamp columns (dates)\n",
    "- Numeric columns (amounts, counts)  \n",
    "- Categorical columns (categories, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "class ColumnType(Enum):\n",
    "    \"\"\"Detected column types.\"\"\"\n",
    "    TIMESTAMP = \"timestamp\"\n",
    "    NUMERIC = \"numeric\"\n",
    "    CATEGORICAL = \"categorical\"\n",
    "    IDENTIFIER = \"identifier\"\n",
    "    TEXT = \"text\"\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "@dataclass\n",
    "class ColumnSchema:\n",
    "    \"\"\"Schema for a single column.\"\"\"\n",
    "    name: str\n",
    "    detected_type: ColumnType\n",
    "    sample_values: List[Any]\n",
    "    null_percentage: float\n",
    "    unique_count: int\n",
    "    is_potential_target: bool\n",
    "    is_potential_feature: bool\n",
    "\n",
    "class SchemaDetector:\n",
    "    \"\"\"Auto-detect data schema from billing data.\"\"\"\n",
    "    \n",
    "    TIMESTAMP_PATTERNS = [\n",
    "        r'\\d{4}-\\d{2}-\\d{2}',  # YYYY-MM-DD\n",
    "        r'\\d{2}/\\d{2}/\\d{4}',  # MM/DD/YYYY\n",
    "        r'\\d{2}-\\d{2}-\\d{4}',  # DD-MM-YYYY\n",
    "        r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)',\n",
    "    ]\n",
    "    \n",
    "    TARGET_KEYWORDS = ['total', 'count', 'amount', 'revenue', 'sales', 'cost', 'price', 'quantity', 'volume']\n",
    "    IDENTIFIER_KEYWORDS = ['id', 'code', 'number', 'key', 'ref', 'uuid']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.timestamp_regex = [re.compile(p, re.IGNORECASE) for p in self.TIMESTAMP_PATTERNS]\n",
    "    \n",
    "    def detect_column_type(self, series: pd.Series, name: str) -> ColumnType:\n",
    "        \"\"\"Detect type of a single column.\"\"\"\n",
    "        name_lower = name.lower()\n",
    "        \n",
    "        # Check for identifiers first\n",
    "        if any(kw in name_lower for kw in self.IDENTIFIER_KEYWORDS):\n",
    "            return ColumnType.IDENTIFIER\n",
    "        \n",
    "        # Try numeric\n",
    "        try:\n",
    "            pd.to_numeric(series.dropna())\n",
    "            return ColumnType.NUMERIC\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "        \n",
    "        # Check for timestamp patterns\n",
    "        sample = str(series.dropna().iloc[0]) if len(series.dropna()) > 0 else \"\"\n",
    "        for pattern in self.timestamp_regex:\n",
    "            if pattern.search(sample):\n",
    "                return ColumnType.TIMESTAMP\n",
    "        \n",
    "        # Try datetime parsing\n",
    "        try:\n",
    "            pd.to_datetime(series.dropna().head(10))\n",
    "            return ColumnType.TIMESTAMP\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Categorical if low cardinality\n",
    "        if series.nunique() < len(series) * 0.5 and series.nunique() < 50:\n",
    "            return ColumnType.CATEGORICAL\n",
    "        \n",
    "        return ColumnType.TEXT\n",
    "    \n",
    "    def analyze(self, df: pd.DataFrame) -> Dict[str, List[str]]:\n",
    "        \"\"\"Analyze DataFrame and return categorized columns.\"\"\"\n",
    "        results = {\n",
    "            'timestamp': [],\n",
    "            'numeric': [],\n",
    "            'categorical': [],\n",
    "            'identifier': [],\n",
    "            'text': [],\n",
    "            'suggested_target': None,\n",
    "            'suggested_time_index': None\n",
    "        }\n",
    "        \n",
    "        for col in df.columns:\n",
    "            col_type = self.detect_column_type(df[col], col)\n",
    "            results[col_type.value].append(col)\n",
    "            \n",
    "            # Suggest target (numeric with target keywords)\n",
    "            if col_type == ColumnType.NUMERIC:\n",
    "                if any(kw in col.lower() for kw in self.TARGET_KEYWORDS):\n",
    "                    results['suggested_target'] = col\n",
    "        \n",
    "        # Suggest time index\n",
    "        if results['timestamp']:\n",
    "            results['suggested_time_index'] = results['timestamp'][0]\n",
    "        \n",
    "        # If no target suggested, use first numeric\n",
    "        if not results['suggested_target'] and results['numeric']:\n",
    "            results['suggested_target'] = results['numeric'][0]\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Apply schema detection\n",
    "detector = SchemaDetector()\n",
    "schema = detector.analyze(df)\n",
    "\n",
    "print(\"üîç SCHEMA DETECTION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in schema.items():\n",
    "    if value:\n",
    "        print(f\"\\n{key.upper()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data based on detected schema\n",
    "def preprocess_billing_data(df: pd.DataFrame, schema: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Clean and preprocess billing data.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Drop rows that are completely empty\n",
    "    df_clean = df_clean.dropna(how='all')\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    for col in schema['numeric']:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "    \n",
    "    # Convert timestamp columns\n",
    "    for col in schema['timestamp']:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "    \n",
    "    # Fill missing values for numeric columns with median\n",
    "    for col in schema['numeric']:\n",
    "        if col in df_clean.columns:\n",
    "            median_val = df_clean[col].median()\n",
    "            df_clean[col] = df_clean[col].fillna(median_val)\n",
    "    \n",
    "    # Fill missing values for categorical columns with mode\n",
    "    for col in schema['categorical']:\n",
    "        if col in df_clean.columns and df_clean[col].notna().any():\n",
    "            mode_val = df_clean[col].mode().iloc[0] if len(df_clean[col].mode()) > 0 else 'Unknown'\n",
    "            df_clean[col] = df_clean[col].fillna(mode_val)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "df_processed = preprocess_billing_data(df, schema)\n",
    "\n",
    "print(\"‚úÖ DATA PREPROCESSING COMPLETE\")\n",
    "print(f\"   Original shape: {df.shape}\")\n",
    "print(f\"   Processed shape: {df_processed.shape}\")\n",
    "print(f\"   Rows removed: {len(df) - len(df_processed)}\")\n",
    "\n",
    "# Show processed data types\n",
    "print(\"\\nüìä Processed Data Types:\")\n",
    "print(df_processed.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1b930",
   "metadata": {},
   "source": [
    "## Section 3: Feature Engineering for Time-Series\n",
    "\n",
    "Create features needed for trend analysis and forecasting:\n",
    "- Time-based features (day of week, month, quarter)\n",
    "- Lag features (previous day, week values)\n",
    "- Rolling averages (7-day, 30-day)\n",
    "- Seasonal indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1087651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(df: pd.DataFrame, time_col: str, value_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create time-series features for trend analysis.\n",
    "    \n",
    "    Features created:\n",
    "    - day_of_week, month, quarter, year\n",
    "    - is_weekend, is_month_start, is_month_end\n",
    "    - lag_1, lag_7, lag_30 (previous values)\n",
    "    - rolling_mean_7, rolling_mean_30\n",
    "    - rolling_std_7 (volatility)\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    if time_col not in df_features.columns:\n",
    "        print(f\"‚ö†Ô∏è Time column '{time_col}' not found. Creating sequential index.\")\n",
    "        df_features['date_index'] = pd.date_range(start='2026-01-01', periods=len(df_features), freq='D')\n",
    "        time_col = 'date_index'\n",
    "    \n",
    "    # Ensure datetime\n",
    "    df_features[time_col] = pd.to_datetime(df_features[time_col], errors='coerce')\n",
    "    \n",
    "    # Sort by time\n",
    "    df_features = df_features.sort_values(time_col).reset_index(drop=True)\n",
    "    \n",
    "    # Time-based features\n",
    "    df_features['day_of_week'] = df_features[time_col].dt.dayofweek\n",
    "    df_features['month'] = df_features[time_col].dt.month\n",
    "    df_features['quarter'] = df_features[time_col].dt.quarter\n",
    "    df_features['year'] = df_features[time_col].dt.year\n",
    "    df_features['day_of_month'] = df_features[time_col].dt.day\n",
    "    df_features['week_of_year'] = df_features[time_col].dt.isocalendar().week\n",
    "    \n",
    "    # Boolean features\n",
    "    df_features['is_weekend'] = df_features['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df_features['is_month_start'] = df_features[time_col].dt.is_month_start.astype(int)\n",
    "    df_features['is_month_end'] = df_features[time_col].dt.is_month_end.astype(int)\n",
    "    \n",
    "    # Lag features (if value column exists)\n",
    "    if value_col and value_col in df_features.columns:\n",
    "        df_features['lag_1'] = df_features[value_col].shift(1)\n",
    "        df_features['lag_7'] = df_features[value_col].shift(7)\n",
    "        df_features['lag_30'] = df_features[value_col].shift(30)\n",
    "        \n",
    "        # Rolling features\n",
    "        df_features['rolling_mean_7'] = df_features[value_col].rolling(window=7, min_periods=1).mean()\n",
    "        df_features['rolling_mean_30'] = df_features[value_col].rolling(window=30, min_periods=1).mean()\n",
    "        df_features['rolling_std_7'] = df_features[value_col].rolling(window=7, min_periods=1).std()\n",
    "        \n",
    "        # Percentage change\n",
    "        df_features['pct_change'] = df_features[value_col].pct_change()\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Get time and value columns from schema\n",
    "time_col = schema.get('suggested_time_index') or (schema['timestamp'][0] if schema['timestamp'] else None)\n",
    "value_col = schema.get('suggested_target')\n",
    "\n",
    "print(f\"üïê Time column: {time_col}\")\n",
    "print(f\"üìä Value column: {value_col}\")\n",
    "\n",
    "# Create features\n",
    "df_features = create_time_features(df_processed, time_col, value_col)\n",
    "\n",
    "print(f\"\\n‚úÖ Features created. New columns:\")\n",
    "new_cols = [c for c in df_features.columns if c not in df_processed.columns]\n",
    "for col in new_cols:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "df_features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c034351c",
   "metadata": {},
   "source": [
    "## Section 4: Train Trend Detection Model\n",
    "\n",
    "Implement TrendDetector from the STATISTICAL_MODELS_IMPLEMENTATION_PLAN:\n",
    "- Linear regression for slope calculation\n",
    "- R¬≤ scoring for trend fit quality\n",
    "- Change point detection for trend shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a266398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "@dataclass\n",
    "class TrendResult:\n",
    "    \"\"\"Results from trend detection.\"\"\"\n",
    "    direction: str  # \"increasing\", \"decreasing\", \"stable\", \"volatile\"\n",
    "    strength: float  # 0-1, how strong the trend is\n",
    "    slope: float  # Rate of change\n",
    "    r_squared: float  # How well the trend fits\n",
    "    change_points: List[int]  # Indices where trend changes\n",
    "    summary: str\n",
    "\n",
    "class TrendDetector:\n",
    "    \"\"\"Detect trends in time-series billing data.\"\"\"\n",
    "    \n",
    "    def analyze(self, values: np.ndarray) -> TrendResult:\n",
    "        \"\"\"Analyze trend in numeric values.\"\"\"\n",
    "        if len(values) < 5:\n",
    "            return TrendResult(\n",
    "                direction=\"unknown\",\n",
    "                strength=0.0,\n",
    "                slope=0.0,\n",
    "                r_squared=0.0,\n",
    "                change_points=[],\n",
    "                summary=\"Insufficient data for trend analysis (minimum 5 points required)\"\n",
    "            )\n",
    "        \n",
    "        # Remove NaN values\n",
    "        values = values[~np.isnan(values)]\n",
    "        \n",
    "        # Calculate trend metrics\n",
    "        direction, strength, slope, r_squared = self._calculate_trend(values)\n",
    "        \n",
    "        # Detect change points\n",
    "        change_points = self._detect_change_points(values)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = self._generate_summary(direction, strength, slope, r_squared, len(values))\n",
    "        \n",
    "        return TrendResult(\n",
    "            direction=direction,\n",
    "            strength=strength,\n",
    "            slope=slope,\n",
    "            r_squared=r_squared,\n",
    "            change_points=change_points,\n",
    "            summary=summary\n",
    "        )\n",
    "    \n",
    "    def _calculate_trend(self, values: np.ndarray) -> Tuple[str, float, float, float]:\n",
    "        \"\"\"Calculate trend direction, strength, slope, and R¬≤.\"\"\"\n",
    "        x = np.arange(len(values)).reshape(-1, 1)\n",
    "        y = values.reshape(-1, 1)\n",
    "        \n",
    "        # Fit linear regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(x, y)\n",
    "        slope = model.coef_[0][0]\n",
    "        \n",
    "        # Calculate R¬≤\n",
    "        y_pred = model.predict(x)\n",
    "        r_squared = r2_score(y, y_pred)\n",
    "        \n",
    "        # Determine direction\n",
    "        mean_val = np.mean(values)\n",
    "        normalized_slope = slope / mean_val if mean_val != 0 else 0\n",
    "        \n",
    "        # Check volatility (coefficient of variation)\n",
    "        cv = np.std(values) / mean_val if mean_val != 0 else 0\n",
    "        \n",
    "        if cv > 0.5:\n",
    "            direction = \"volatile\"\n",
    "            strength = min(cv, 1.0)\n",
    "        elif abs(normalized_slope) < 0.01:\n",
    "            direction = \"stable\"\n",
    "            strength = 1 - abs(normalized_slope) * 10\n",
    "        elif normalized_slope > 0:\n",
    "            direction = \"increasing\"\n",
    "            strength = min(abs(normalized_slope) * 10, 1)\n",
    "        else:\n",
    "            direction = \"decreasing\"\n",
    "            strength = min(abs(normalized_slope) * 10, 1)\n",
    "        \n",
    "        return direction, strength, slope, r_squared\n",
    "    \n",
    "    def _detect_change_points(self, values: np.ndarray, sensitivity: float = 2.0) -> List[int]:\n",
    "        \"\"\"Detect points where the trend changes significantly.\"\"\"\n",
    "        if len(values) < 10:\n",
    "            return []\n",
    "        \n",
    "        change_points = []\n",
    "        window = min(5, len(values) // 3)\n",
    "        \n",
    "        rolling_mean = pd.Series(values).rolling(window).mean().values\n",
    "        rolling_std = pd.Series(values).rolling(window).std().values\n",
    "        \n",
    "        for i in range(window, len(values) - 1):\n",
    "            if rolling_std[i] > 0:\n",
    "                z_score = abs(values[i] - rolling_mean[i]) / rolling_std[i]\n",
    "                if z_score > sensitivity:\n",
    "                    change_points.append(i)\n",
    "        \n",
    "        return change_points\n",
    "    \n",
    "    def _generate_summary(self, direction: str, strength: float, slope: float, \n",
    "                         r_squared: float, data_points: int) -> str:\n",
    "        \"\"\"Generate human-readable trend summary.\"\"\"\n",
    "        summary = f\"Based on {data_points} data points, the trend is {direction}\"\n",
    "        \n",
    "        if direction in [\"increasing\", \"decreasing\"]:\n",
    "            summary += f\" with {strength*100:.0f}% confidence\"\n",
    "            summary += f\". R¬≤ fit: {r_squared:.3f}\"\n",
    "            summary += f\". Slope: {slope:.4f} per period\"\n",
    "        elif direction == \"volatile\":\n",
    "            summary += f\". High variability detected (CV > 50%)\"\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Apply trend detection to numeric columns\n",
    "trend_detector = TrendDetector()\n",
    "\n",
    "print(\"üìà TREND ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for col in schema['numeric'][:5]:  # Analyze top 5 numeric columns\n",
    "    if col in df_features.columns:\n",
    "        values = df_features[col].dropna().values\n",
    "        if len(values) >= 5:\n",
    "            result = trend_detector.analyze(values)\n",
    "            print(f\"\\nüìä {col}:\")\n",
    "            print(f\"   Direction: {result.direction}\")\n",
    "            print(f\"   Strength: {result.strength:.2%}\")\n",
    "            print(f\"   R¬≤ Score: {result.r_squared:.3f}\")\n",
    "            print(f\"   Change Points: {len(result.change_points)}\")\n",
    "            print(f\"   Summary: {result.summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0be743",
   "metadata": {},
   "source": [
    "## Section 5: Train Anomaly Detection Model\n",
    "\n",
    "Implement anomaly detection using:\n",
    "1. **Isolation Forest** - ML-based outlier detection\n",
    "2. **Z-Score** - Statistical deviation detection\n",
    "3. **IQR** - Interquartile range method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "@dataclass\n",
    "class Anomaly:\n",
    "    \"\"\"Represents a detected anomaly.\"\"\"\n",
    "    index: int\n",
    "    value: Any\n",
    "    anomaly_score: float\n",
    "    severity: str  # \"low\", \"medium\", \"high\"\n",
    "    explanation: str\n",
    "\n",
    "@dataclass\n",
    "class AnomalyResult:\n",
    "    \"\"\"Results from anomaly detection.\"\"\"\n",
    "    anomalies: List[Anomaly]\n",
    "    total_records: int\n",
    "    anomaly_percentage: float\n",
    "    method_used: str\n",
    "    summary: str\n",
    "\n",
    "class AnomalyDetector:\n",
    "    \"\"\"Multi-method anomaly detection for billing data.\"\"\"\n",
    "    \n",
    "    def __init__(self, contamination: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            contamination: Expected proportion of anomalies (default 10%)\n",
    "        \"\"\"\n",
    "        self.contamination = contamination\n",
    "    \n",
    "    def detect_isolation_forest(self, df: pd.DataFrame, columns: List[str]) -> AnomalyResult:\n",
    "        \"\"\"Detect anomalies using Isolation Forest.\"\"\"\n",
    "        # Prepare data\n",
    "        X = df[columns].fillna(df[columns].median())\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Fit Isolation Forest\n",
    "        model = IsolationForest(\n",
    "            contamination=self.contamination,\n",
    "            random_state=42,\n",
    "            n_estimators=100\n",
    "        )\n",
    "        predictions = model.fit_predict(X_scaled)\n",
    "        scores = model.decision_function(X_scaled)\n",
    "        \n",
    "        # Collect anomalies\n",
    "        anomalies = []\n",
    "        for i, (pred, score) in enumerate(zip(predictions, scores)):\n",
    "            if pred == -1:  # Anomaly\n",
    "                severity = \"high\" if score < -0.3 else \"medium\" if score < -0.1 else \"low\"\n",
    "                anomalies.append(Anomaly(\n",
    "                    index=i,\n",
    "                    value=dict(df[columns].iloc[i]),\n",
    "                    anomaly_score=abs(score),\n",
    "                    severity=severity,\n",
    "                    explanation=f\"Isolation Forest score: {score:.3f}\"\n",
    "                ))\n",
    "        \n",
    "        return AnomalyResult(\n",
    "            anomalies=sorted(anomalies, key=lambda x: x.anomaly_score, reverse=True),\n",
    "            total_records=len(df),\n",
    "            anomaly_percentage=len(anomalies) / len(df) * 100,\n",
    "            method_used=\"Isolation Forest\",\n",
    "            summary=f\"Found {len(anomalies)} anomalies ({len(anomalies)/len(df)*100:.1f}%) using Isolation Forest\"\n",
    "        )\n",
    "    \n",
    "    def detect_zscore(self, df: pd.DataFrame, columns: List[str], threshold: float = 3.0) -> AnomalyResult:\n",
    "        \"\"\"Detect anomalies using Z-Score method.\"\"\"\n",
    "        anomalies = []\n",
    "        \n",
    "        for col in columns:\n",
    "            values = df[col].dropna()\n",
    "            mean = values.mean()\n",
    "            std = values.std()\n",
    "            \n",
    "            if std == 0:\n",
    "                continue\n",
    "            \n",
    "            for idx in values.index:\n",
    "                z = abs((values[idx] - mean) / std)\n",
    "                if z > threshold:\n",
    "                    severity = \"high\" if z > 4 else \"medium\" if z > 3.5 else \"low\"\n",
    "                    anomalies.append(Anomaly(\n",
    "                        index=idx,\n",
    "                        value={col: values[idx]},\n",
    "                        anomaly_score=z,\n",
    "                        severity=severity,\n",
    "                        explanation=f\"Z-score {z:.2f} exceeds threshold {threshold} for {col}\"\n",
    "                    ))\n",
    "        \n",
    "        # Remove duplicates by index\n",
    "        seen_indices = set()\n",
    "        unique_anomalies = []\n",
    "        for a in anomalies:\n",
    "            if a.index not in seen_indices:\n",
    "                seen_indices.add(a.index)\n",
    "                unique_anomalies.append(a)\n",
    "        \n",
    "        return AnomalyResult(\n",
    "            anomalies=sorted(unique_anomalies, key=lambda x: x.anomaly_score, reverse=True),\n",
    "            total_records=len(df),\n",
    "            anomaly_percentage=len(unique_anomalies) / len(df) * 100,\n",
    "            method_used=\"Z-Score\",\n",
    "            summary=f\"Found {len(unique_anomalies)} anomalies ({len(unique_anomalies)/len(df)*100:.1f}%) using Z-Score (threshold={threshold})\"\n",
    "        )\n",
    "\n",
    "# Apply anomaly detection\n",
    "anomaly_detector = AnomalyDetector(contamination=0.1)\n",
    "\n",
    "numeric_cols = [c for c in schema['numeric'] if c in df_features.columns]\n",
    "\n",
    "if numeric_cols:\n",
    "    print(\"üîç ANOMALY DETECTION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Isolation Forest\n",
    "    if_result = anomaly_detector.detect_isolation_forest(df_features, numeric_cols)\n",
    "    print(f\"\\nüå≤ {if_result.summary}\")\n",
    "    print(f\"   Top anomalies by score:\")\n",
    "    for a in if_result.anomalies[:5]:\n",
    "        print(f\"      Row {a.index}: {a.severity.upper()} - Score: {a.anomaly_score:.3f}\")\n",
    "    \n",
    "    # Z-Score\n",
    "    zs_result = anomaly_detector.detect_zscore(df_features, numeric_cols)\n",
    "    print(f\"\\nüìä {zs_result.summary}\")\n",
    "    print(f\"   Top anomalies by Z-score:\")\n",
    "    for a in zs_result.anomalies[:5]:\n",
    "        print(f\"      Row {a.index}: {a.severity.upper()} - Z={a.anomaly_score:.2f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No numeric columns available for anomaly detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17ac58",
   "metadata": {},
   "source": [
    "## Section 6: Build Knowledge Base with Embeddings\n",
    "\n",
    "For RAG-based Q&A, we need to:\n",
    "1. Convert billing records to text descriptions\n",
    "2. Generate embeddings using sentence transformers\n",
    "3. Store embeddings for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: For production, use your existing embedding API or install sentence-transformers\n",
    "# pip install sentence-transformers\n",
    "\n",
    "def row_to_text(row: pd.Series, columns: List[str]) -> str:\n",
    "    \"\"\"Convert a DataFrame row to natural language text for embedding.\"\"\"\n",
    "    parts = []\n",
    "    for col in columns:\n",
    "        if pd.notna(row.get(col)):\n",
    "            # Clean column name\n",
    "            clean_col = col.replace('_', ' ').title()\n",
    "            value = row[col]\n",
    "            \n",
    "            # Format based on type\n",
    "            if isinstance(value, float):\n",
    "                value = f\"{value:,.2f}\"\n",
    "            elif isinstance(value, pd.Timestamp):\n",
    "                value = value.strftime(\"%B %d, %Y\")\n",
    "            \n",
    "            parts.append(f\"{clean_col}: {value}\")\n",
    "    \n",
    "    return \". \".join(parts)\n",
    "\n",
    "def create_knowledge_base_text(df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Create text representations of billing records for RAG.\"\"\"\n",
    "    knowledge_base = []\n",
    "    \n",
    "    columns_to_use = [c for c in df.columns if not c.startswith('lag_') and not c.startswith('rolling_')]\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        text = row_to_text(row, columns_to_use)\n",
    "        knowledge_base.append({\n",
    "            'id': idx,\n",
    "            'text': text,\n",
    "            'source': 'billing_data',\n",
    "            'row_data': row.to_dict()\n",
    "        })\n",
    "    \n",
    "    return knowledge_base\n",
    "\n",
    "# Create knowledge base\n",
    "kb = create_knowledge_base_text(df_features.head(50))  # Sample for demo\n",
    "\n",
    "print(\"üìö KNOWLEDGE BASE PREVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total documents: {len(kb)}\")\n",
    "print(f\"\\nüìÑ Sample document:\")\n",
    "print(f\"   ID: {kb[0]['id']}\")\n",
    "print(f\"   Text: {kb[0]['text'][:200]}...\")\n",
    "\n",
    "# Embedding strategy note\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîó EMBEDDING STRATEGY FOR PRODUCTION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "For BillingRag, embeddings are generated via:\n",
    "1. External Embedding API (current production approach)\n",
    "2. Alternative: sentence-transformers locally\n",
    "\n",
    "Code to generate embeddings (when API not available):\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load model (~400MB)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "texts = [doc['text'] for doc in kb]\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Store in MongoDB with vectors\n",
    "for doc, emb in zip(kb, embeddings):\n",
    "    doc['embedding'] = emb.tolist()\n",
    "```\n",
    "\n",
    "Storage: MongoDB Atlas with $vectorSearch index\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8892565",
   "metadata": {},
   "source": [
    "## Section 7: Model Evaluation and Metrics\n",
    "\n",
    "Evaluate models against success criteria from STATISTICAL_MODELS_IMPLEMENTATION_PLAN.md:\n",
    "- ‚úÖ Schema auto-detection accuracy > 90%\n",
    "- ‚úÖ Anomaly detection precision > 80%\n",
    "- ‚úÖ Forecast MAPE < 20%\n",
    "- ‚úÖ Response time < 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87281af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_models(df: pd.DataFrame, schema: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    results = {\n",
    "        'schema_detection': {},\n",
    "        'trend_detection': {},\n",
    "        'anomaly_detection': {},\n",
    "        'forecasting': {},\n",
    "        'performance': {}\n",
    "    }\n",
    "    \n",
    "    # 1. Schema Detection Accuracy\n",
    "    # Check if detected types match pandas inferred types\n",
    "    schema_matches = 0\n",
    "    total_cols = len(df.columns)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        detected = None\n",
    "        if col in schema['numeric']:\n",
    "            detected = 'numeric'\n",
    "        elif col in schema['timestamp']:\n",
    "            detected = 'timestamp'\n",
    "        elif col in schema['categorical']:\n",
    "            detected = 'categorical'\n",
    "        \n",
    "        # Verify against pandas dtype\n",
    "        actual = str(df[col].dtype)\n",
    "        if detected == 'numeric' and 'float' in actual or 'int' in actual:\n",
    "            schema_matches += 1\n",
    "        elif detected == 'timestamp' and 'datetime' in actual:\n",
    "            schema_matches += 1\n",
    "        elif detected == 'categorical' and 'object' in actual:\n",
    "            schema_matches += 1\n",
    "        elif detected is None:\n",
    "            schema_matches += 0.5  # Partial credit\n",
    "    \n",
    "    results['schema_detection'] = {\n",
    "        'accuracy': schema_matches / total_cols * 100 if total_cols > 0 else 0,\n",
    "        'columns_detected': total_cols,\n",
    "        'meets_criteria': (schema_matches / total_cols * 100) > 90 if total_cols > 0 else False\n",
    "    }\n",
    "    \n",
    "    # 2. Trend Detection Performance\n",
    "    numeric_cols = [c for c in schema['numeric'] if c in df.columns]\n",
    "    if numeric_cols:\n",
    "        start_time = time.time()\n",
    "        detector = TrendDetector()\n",
    "        \n",
    "        for col in numeric_cols[:3]:\n",
    "            values = df[col].dropna().values\n",
    "            if len(values) >= 5:\n",
    "                result = detector.analyze(values)\n",
    "        \n",
    "        trend_time = time.time() - start_time\n",
    "        \n",
    "        results['trend_detection'] = {\n",
    "            'columns_analyzed': len(numeric_cols[:3]),\n",
    "            'processing_time_ms': trend_time * 1000,\n",
    "            'meets_time_criteria': trend_time < 5\n",
    "        }\n",
    "    \n",
    "    # 3. Anomaly Detection Precision\n",
    "    if numeric_cols:\n",
    "        start_time = time.time()\n",
    "        anomaly_detector = AnomalyDetector(contamination=0.1)\n",
    "        \n",
    "        # We estimate precision based on score distribution\n",
    "        if_result = anomaly_detector.detect_isolation_forest(df, numeric_cols)\n",
    "        \n",
    "        # High confidence anomalies (estimated precision)\n",
    "        high_conf = [a for a in if_result.anomalies if a.severity == 'high']\n",
    "        estimated_precision = len(high_conf) / len(if_result.anomalies) if if_result.anomalies else 1.0\n",
    "        \n",
    "        anomaly_time = time.time() - start_time\n",
    "        \n",
    "        results['anomaly_detection'] = {\n",
    "            'total_anomalies': len(if_result.anomalies),\n",
    "            'high_confidence': len(high_conf),\n",
    "            'estimated_precision': estimated_precision * 100,\n",
    "            'processing_time_ms': anomaly_time * 1000,\n",
    "            'meets_precision_criteria': estimated_precision > 0.8\n",
    "        }\n",
    "    \n",
    "    # 4. Overall Performance\n",
    "    results['performance'] = {\n",
    "        'total_records': len(df),\n",
    "        'total_features': len(df.columns),\n",
    "        'memory_mb': df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_models(df_features, schema)\n",
    "\n",
    "print(\"üìä MODEL EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ SCHEMA DETECTION:\")\n",
    "sd = eval_results['schema_detection']\n",
    "print(f\"   Accuracy: {sd['accuracy']:.1f}%\")\n",
    "print(f\"   Criteria (>90%): {'‚úÖ PASS' if sd['meets_criteria'] else '‚ùå FAIL'}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ TREND DETECTION:\")\n",
    "td = eval_results['trend_detection']\n",
    "print(f\"   Columns Analyzed: {td.get('columns_analyzed', 0)}\")\n",
    "print(f\"   Processing Time: {td.get('processing_time_ms', 0):.2f}ms\")\n",
    "print(f\"   Criteria (<5s): {'‚úÖ PASS' if td.get('meets_time_criteria', False) else '‚ùå FAIL'}\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ ANOMALY DETECTION:\")\n",
    "ad = eval_results['anomaly_detection']\n",
    "print(f\"   Total Anomalies: {ad.get('total_anomalies', 0)}\")\n",
    "print(f\"   High Confidence: {ad.get('high_confidence', 0)}\")\n",
    "print(f\"   Est. Precision: {ad.get('estimated_precision', 0):.1f}%\")\n",
    "print(f\"   Criteria (>80%): {'‚úÖ PASS' if ad.get('meets_precision_criteria', False) else '‚ùå FAIL'}\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ PERFORMANCE:\")\n",
    "perf = eval_results['performance']\n",
    "print(f\"   Total Records: {perf['total_records']}\")\n",
    "print(f\"   Memory Usage: {perf['memory_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce1d636",
   "metadata": {},
   "source": [
    "## Section 8: Model Serialization and Storage Options\n",
    "\n",
    "Compare different options for storing trained ML models:\n",
    "1. **Local pickle/joblib** - Simple, fast\n",
    "2. **MongoDB GridFS** - Already using MongoDB\n",
    "3. **Azure Blob Storage** - Scalable, enterprise\n",
    "4. **MLflow Model Registry** - Full ML lifecycle\n",
    "5. **Hugging Face Hub** - Open source, community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e855dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class ModelStorage:\n",
    "    \"\"\"Model serialization and storage utilities.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_pickle(model: Any, filepath: str, metadata: Dict = None) -> str:\n",
    "        \"\"\"Save model using pickle (fastest for sklearn models).\"\"\"\n",
    "        save_data = {\n",
    "            'model': model,\n",
    "            'metadata': metadata or {},\n",
    "            'saved_at': datetime.now().isoformat(),\n",
    "            'version': '1.0'\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "        \n",
    "        return filepath\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_joblib(model: Any, filepath: str, metadata: Dict = None) -> str:\n",
    "        \"\"\"Save model using joblib (better for numpy arrays).\"\"\"\n",
    "        save_data = {\n",
    "            'model': model,\n",
    "            'metadata': metadata or {},\n",
    "            'saved_at': datetime.now().isoformat(),\n",
    "            'version': '1.0'\n",
    "        }\n",
    "        \n",
    "        joblib.dump(save_data, filepath, compress=3)\n",
    "        return filepath\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_model(filepath: str) -> Tuple[Any, Dict]:\n",
    "        \"\"\"Load model from pickle or joblib file.\"\"\"\n",
    "        if filepath.endswith('.joblib'):\n",
    "            data = joblib.load(filepath)\n",
    "        else:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "        \n",
    "        return data['model'], data.get('metadata', {})\n",
    "\n",
    "# Example: Save the anomaly detection model\n",
    "if numeric_cols:\n",
    "    # Train a fresh model for saving\n",
    "    save_model = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
    "    X = df_features[numeric_cols].fillna(df_features[numeric_cols].median())\n",
    "    save_model.fit(X)\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        'model_type': 'IsolationForest',\n",
    "        'training_rows': len(X),\n",
    "        'features': numeric_cols,\n",
    "        'contamination': 0.1,\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'data_source': 'Export of Trending Dashboard Report_TAB5_dec 1.xlsx'\n",
    "    }\n",
    "    \n",
    "    # Save using both methods\n",
    "    storage = ModelStorage()\n",
    "    \n",
    "    pickle_path = storage.save_pickle(save_model, 'anomaly_detector.pkl', metadata)\n",
    "    joblib_path = storage.save_joblib(save_model, 'anomaly_detector.joblib', metadata)\n",
    "    \n",
    "    print(\"üíæ MODEL SERIALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n‚úÖ Pickle saved: {pickle_path}\")\n",
    "    print(f\"‚úÖ Joblib saved: {joblib_path}\")\n",
    "    \n",
    "    # Compare file sizes\n",
    "    import os\n",
    "    print(f\"\\nüì¶ File Sizes:\")\n",
    "    print(f\"   Pickle: {os.path.getsize(pickle_path) / 1024:.2f} KB\")\n",
    "    print(f\"   Joblib: {os.path.getsize(joblib_path) / 1024:.2f} KB\")\n",
    "    \n",
    "    # Test loading\n",
    "    loaded_model, loaded_meta = storage.load_model(joblib_path)\n",
    "    print(f\"\\n‚úÖ Model loaded successfully\")\n",
    "    print(f\"   Type: {loaded_meta.get('model_type')}\")\n",
    "    print(f\"   Trained on: {loaded_meta.get('training_rows')} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05feeda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage Options Comparison\n",
    "print(\"üóÑÔ∏è ML MODEL STORAGE OPTIONS COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "storage_options = [\n",
    "    {\n",
    "        'name': 'Local Pickle/Joblib',\n",
    "        'pros': ['Fast', 'Simple', 'No dependencies', 'Good for sklearn'],\n",
    "        'cons': ['Not scalable', 'No versioning', 'Manual backup needed'],\n",
    "        'cost': 'Free',\n",
    "        'best_for': 'Development, small deployments',\n",
    "        'code': \"joblib.dump(model, 'model.joblib')\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'MongoDB GridFS',\n",
    "        'pros': ['Already using MongoDB', 'Automatic replication', 'Large file support'],\n",
    "        'cons': ['Extra complexity', 'Not designed for ML', 'Query overhead'],\n",
    "        'cost': 'Existing MongoDB cost',\n",
    "        'best_for': 'BillingRag (existing infra)',\n",
    "        'code': \"fs.put(model_bytes, filename='model.pkl')\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'Azure Blob Storage',\n",
    "        'pros': ['Scalable', 'Enterprise ready', 'Lifecycle management', 'CDN support'],\n",
    "        'cons': ['Extra Azure cost', 'Network latency', 'Setup complexity'],\n",
    "        'cost': '~$0.02/GB/month',\n",
    "        'best_for': 'Production enterprise deployments',\n",
    "        'code': \"blob_client.upload_blob(model_bytes)\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'MLflow Model Registry',\n",
    "        'pros': ['Full ML lifecycle', 'Versioning', 'Staging/Production', 'Model comparison'],\n",
    "        'cons': ['Learning curve', 'Server needed', 'Overkill for simple cases'],\n",
    "        'cost': 'Free (self-hosted) or Databricks cost',\n",
    "        'best_for': 'ML teams, model governance',\n",
    "        'code': \"mlflow.sklearn.log_model(model, 'model')\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'Hugging Face Hub',\n",
    "        'pros': ['Free hosting', 'Community', 'Easy sharing', 'Model cards'],\n",
    "        'cons': ['Public by default', 'Limited for proprietary', 'Slower loads'],\n",
    "        'cost': 'Free (public) / $9/mo (private)',\n",
    "        'best_for': 'Open source, NLP models',\n",
    "        'code': \"model.push_to_hub('username/model')\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, opt in enumerate(storage_options, 1):\n",
    "    print(f\"\\n{i}. {opt['name']}\")\n",
    "    print(f\"   Cost: {opt['cost']}\")\n",
    "    print(f\"   Best for: {opt['best_for']}\")\n",
    "    print(f\"   ‚úÖ Pros: {', '.join(opt['pros'])}\")\n",
    "    print(f\"   ‚ùå Cons: {', '.join(opt['cons'])}\")\n",
    "    print(f\"   üìù Example: {opt['code']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ RECOMMENDATION FOR BILLINGRAG:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "PRIMARY: MongoDB GridFS (you already have MongoDB Atlas)\n",
    "- Store models as binary in GridFS\n",
    "- Use metadata collection for versioning\n",
    "- Automatic replication with Atlas\n",
    "\n",
    "SECONDARY: Azure Blob Storage (for larger models/backups)\n",
    "- Use for Prophet/ARIMA models (larger)\n",
    "- Lifecycle policies for old versions\n",
    "- Integrate with existing Azure infrastructure\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa6b635",
   "metadata": {},
   "source": [
    "## Section 9: Replit Integration - Can We Use It?\n",
    "\n",
    "**Replit** is a cloud IDE with deployment capabilities. Let's evaluate if it's suitable for BillingRag ML deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß REPLIT EVALUATION FOR BILLINGRAG ML\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "replit_analysis = {\n",
    "    'free_tier': {\n",
    "        'cpu': '0.5 vCPU',\n",
    "        'ram': '512 MB',\n",
    "        'storage': '1 GB',\n",
    "        'egress': '10 GB/month',\n",
    "        'always_on': 'No (sleeps after inactivity)',\n",
    "        'suitable': '‚ùå NOT SUITABLE for ML'\n",
    "    },\n",
    "    'hacker_plan': {\n",
    "        'cost': '$7/month',\n",
    "        'cpu': '2 vCPU',\n",
    "        'ram': '2 GB',\n",
    "        'storage': '10 GB',\n",
    "        'always_on': 'Yes (limited hours)',\n",
    "        'suitable': '‚ö†Ô∏è MARGINAL for small models'\n",
    "    },\n",
    "    'pro_plan': {\n",
    "        'cost': '$20/month',\n",
    "        'cpu': '4 vCPU',\n",
    "        'ram': '8 GB',\n",
    "        'storage': '50 GB',\n",
    "        'always_on': 'Yes',\n",
    "        'suitable': '‚úÖ OK for development/testing'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nüìä REPLIT PLAN COMPARISON:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for plan, details in replit_analysis.items():\n",
    "    print(f\"\\n{plan.upper().replace('_', ' ')}:\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"   {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚ö†Ô∏è REPLIT LIMITATIONS FOR ML:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "1. MEMORY: sklearn models need 1-4GB RAM; Prophet needs 2-4GB\n",
    "   - Free tier (512MB) = Will crash on model training\n",
    "   - Hacker (2GB) = Marginal for small models\n",
    "   - Pro (8GB) = Acceptable for most use cases\n",
    "\n",
    "2. COLD STARTS: Free tier sleeps after 30min inactivity\n",
    "   - First request takes 20-60 seconds to wake up\n",
    "   - Bad UX for production API\n",
    "\n",
    "3. NO GPU: Cannot use GPU-accelerated models\n",
    "   - No PyTorch/TensorFlow GPU support\n",
    "   - Embedding generation will be slow\n",
    "\n",
    "4. STORAGE: Ephemeral by default\n",
    "   - Models must be stored externally (S3, MongoDB)\n",
    "   - Re-download on each cold start\n",
    "\n",
    "5. ENTERPRISE: No SOC2, HIPAA compliance\n",
    "   - Not suitable for sensitive billing data\n",
    "\n",
    "6. NETWORKING: Limited outbound connections\n",
    "   - May have issues with MongoDB Atlas, Azure APIs\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ VERDICT: CAN WE USE REPLIT?\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "FOR BILLINGRAG ML:\n",
    "\n",
    "‚ùå NOT RECOMMENDED FOR PRODUCTION because:\n",
    "   - Insufficient memory for Prophet/ARIMA training\n",
    "   - Cold starts hurt user experience\n",
    "   - No enterprise compliance\n",
    "   - You already have Kubernetes infrastructure\n",
    "\n",
    "‚úÖ CAN USE FOR:\n",
    "   - Quick prototyping/demos\n",
    "   - Testing API endpoints\n",
    "   - Documentation examples\n",
    "   - Training notebooks (like this one)\n",
    "\n",
    "üöÄ BETTER ALTERNATIVES:\n",
    "\"\"\")\n",
    "\n",
    "alternatives = [\n",
    "    ('Railway', '$5/month', '8GB RAM, auto-deploy from GitHub, no cold starts'),\n",
    "    ('Render', '$7/month', '2GB RAM, auto-deploy, free tier available'),\n",
    "    ('Fly.io', '$5/month', '1GB RAM, global edge, fast cold starts'),\n",
    "    ('Your K8s Cluster', 'Existing', 'Full control, already deployed, best option'),\n",
    "    ('Azure Container Apps', '$10/month', 'Serverless, auto-scale, Azure integration'),\n",
    "]\n",
    "\n",
    "print(f\"{'Platform':<20} {'Cost':<12} {'Features'}\")\n",
    "print(\"-\" * 80)\n",
    "for platform, cost, features in alternatives:\n",
    "    print(f\"{platform:<20} {cost:<12} {features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d06843f",
   "metadata": {},
   "source": [
    "## Section 10: Monthly Retraining Pipeline\n",
    "\n",
    "Design an automated pipeline for:\n",
    "1. Monthly data ingestion\n",
    "2. Model retraining\n",
    "3. Evaluation comparison\n",
    "4. Model versioning with rollback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3fc305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import hashlib\n",
    "\n",
    "@dataclass\n",
    "class ModelVersion:\n",
    "    \"\"\"Represents a versioned model.\"\"\"\n",
    "    version: str\n",
    "    model_path: str\n",
    "    metrics: Dict[str, float]\n",
    "    created_at: str\n",
    "    training_data_hash: str\n",
    "    is_active: bool = False\n",
    "\n",
    "class MonthlyRetrainingPipeline:\n",
    "    \"\"\"\n",
    "    Automated pipeline for monthly model retraining.\n",
    "    \n",
    "    Pipeline Steps:\n",
    "    1. Load new monthly data\n",
    "    2. Preprocess and validate\n",
    "    3. Train new model\n",
    "    4. Evaluate against current model\n",
    "    5. Promote if better, rollback if worse\n",
    "    6. Archive old versions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir: str = \"./models\"):\n",
    "        self.model_dir = Path(model_dir)\n",
    "        self.model_dir.mkdir(exist_ok=True)\n",
    "        self.versions: List[ModelVersion] = []\n",
    "        self.min_improvement = 0.05  # 5% improvement required to promote\n",
    "    \n",
    "    def compute_data_hash(self, df: pd.DataFrame) -> str:\n",
    "        \"\"\"Create hash of training data for tracking.\"\"\"\n",
    "        data_str = df.to_json()\n",
    "        return hashlib.md5(data_str.encode()).hexdigest()[:8]\n",
    "    \n",
    "    def train_new_model(self, df: pd.DataFrame, columns: List[str]) -> Tuple[Any, Dict[str, float]]:\n",
    "        \"\"\"Train a new model on the provided data.\"\"\"\n",
    "        # Prepare data\n",
    "        X = df[columns].fillna(df[columns].median())\n",
    "        \n",
    "        # Train Isolation Forest\n",
    "        model = IsolationForest(\n",
    "            contamination=0.1,\n",
    "            random_state=42,\n",
    "            n_estimators=100\n",
    "        )\n",
    "        model.fit(X)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        predictions = model.predict(X)\n",
    "        scores = model.decision_function(X)\n",
    "        \n",
    "        metrics = {\n",
    "            'anomaly_rate': (predictions == -1).mean(),\n",
    "            'mean_score': scores.mean(),\n",
    "            'std_score': scores.std(),\n",
    "            'training_samples': len(X)\n",
    "        }\n",
    "        \n",
    "        return model, metrics\n",
    "    \n",
    "    def compare_models(self, old_metrics: Dict, new_metrics: Dict) -> bool:\n",
    "        \"\"\"Compare metrics to decide if new model is better.\"\"\"\n",
    "        # For anomaly detection, we want consistent anomaly rate\n",
    "        # and better separation (higher std_score)\n",
    "        \n",
    "        old_quality = old_metrics.get('std_score', 0)\n",
    "        new_quality = new_metrics.get('std_score', 0)\n",
    "        \n",
    "        improvement = (new_quality - old_quality) / old_quality if old_quality > 0 else 1.0\n",
    "        \n",
    "        return improvement > self.min_improvement\n",
    "    \n",
    "    def run_pipeline(self, new_data: pd.DataFrame, columns: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the full retraining pipeline.\n",
    "        \n",
    "        Returns:\n",
    "            Pipeline execution results\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'status': 'started',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'steps': []\n",
    "        }\n",
    "        \n",
    "        # Step 1: Data Validation\n",
    "        results['steps'].append({\n",
    "            'step': 'data_validation',\n",
    "            'rows': len(new_data),\n",
    "            'columns': len(columns),\n",
    "            'status': 'success'\n",
    "        })\n",
    "        \n",
    "        # Step 2: Compute data hash\n",
    "        data_hash = self.compute_data_hash(new_data[columns])\n",
    "        results['data_hash'] = data_hash\n",
    "        \n",
    "        # Step 3: Train new model\n",
    "        new_model, new_metrics = self.train_new_model(new_data, columns)\n",
    "        results['steps'].append({\n",
    "            'step': 'training',\n",
    "            'metrics': new_metrics,\n",
    "            'status': 'success'\n",
    "        })\n",
    "        \n",
    "        # Step 4: Compare with current model\n",
    "        should_promote = True\n",
    "        if self.versions:\n",
    "            current = [v for v in self.versions if v.is_active]\n",
    "            if current:\n",
    "                old_metrics = current[0].metrics\n",
    "                should_promote = self.compare_models(old_metrics, new_metrics)\n",
    "        \n",
    "        results['steps'].append({\n",
    "            'step': 'comparison',\n",
    "            'should_promote': should_promote,\n",
    "            'status': 'success'\n",
    "        })\n",
    "        \n",
    "        # Step 5: Save and version\n",
    "        version_id = f\"v{len(self.versions) + 1}_{data_hash}\"\n",
    "        model_path = self.model_dir / f\"model_{version_id}.joblib\"\n",
    "        \n",
    "        joblib.dump({\n",
    "            'model': new_model,\n",
    "            'metrics': new_metrics,\n",
    "            'columns': columns\n",
    "        }, model_path)\n",
    "        \n",
    "        # Create version record\n",
    "        new_version = ModelVersion(\n",
    "            version=version_id,\n",
    "            model_path=str(model_path),\n",
    "            metrics=new_metrics,\n",
    "            created_at=datetime.now().isoformat(),\n",
    "            training_data_hash=data_hash,\n",
    "            is_active=should_promote\n",
    "        )\n",
    "        \n",
    "        # Deactivate old versions if promoting\n",
    "        if should_promote:\n",
    "            for v in self.versions:\n",
    "                v.is_active = False\n",
    "        \n",
    "        self.versions.append(new_version)\n",
    "        \n",
    "        results['steps'].append({\n",
    "            'step': 'versioning',\n",
    "            'version_id': version_id,\n",
    "            'is_active': should_promote,\n",
    "            'status': 'success'\n",
    "        })\n",
    "        \n",
    "        results['status'] = 'completed'\n",
    "        results['promoted'] = should_promote\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def rollback(self, version_id: str) -> bool:\n",
    "        \"\"\"Rollback to a specific version.\"\"\"\n",
    "        for v in self.versions:\n",
    "            if v.version == version_id:\n",
    "                for other in self.versions:\n",
    "                    other.is_active = False\n",
    "                v.is_active = True\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Demo the pipeline\n",
    "print(\"üîÑ MONTHLY RETRAINING PIPELINE DEMO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if numeric_cols:\n",
    "    pipeline = MonthlyRetrainingPipeline(model_dir=\"./models\")\n",
    "    \n",
    "    # Simulate running pipeline with current data\n",
    "    results = pipeline.run_pipeline(df_features, numeric_cols)\n",
    "    \n",
    "    print(f\"\\nüìã Pipeline Status: {results['status'].upper()}\")\n",
    "    print(f\"‚è∞ Timestamp: {results['timestamp']}\")\n",
    "    print(f\"üîë Data Hash: {results['data_hash']}\")\n",
    "    \n",
    "    print(f\"\\nüìä Pipeline Steps:\")\n",
    "    for step in results['steps']:\n",
    "        status_icon = \"‚úÖ\" if step['status'] == 'success' else \"‚ùå\"\n",
    "        print(f\"   {status_icon} {step['step'].replace('_', ' ').title()}\")\n",
    "        if 'metrics' in step:\n",
    "            for k, v in step['metrics'].items():\n",
    "                print(f\"      - {k}: {v:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Model Promoted: {'Yes' if results['promoted'] else 'No'}\")\n",
    "    print(f\"üì¶ Version Created: {results['steps'][-1]['version_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ce6b8",
   "metadata": {},
   "source": [
    "## üìä Final Feasibility Assessment\n",
    "\n",
    "Based on the analysis, here's the complete feasibility assessment for training ML models on monthly billing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üìä FINAL FEASIBILITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "assessment = \"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    BILLINGRAG ML FEASIBILITY SUMMARY                        ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ  üìÅ DATA ASSESSMENT                                                          ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                           ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Monthly billing data: ‚úÖ SUITABLE for ML                                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Data volume: ~100-500 rows/month (adequate for statistical models)        ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Data quality: Needs preprocessing (nulls, type conversion)                ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Schema consistency: Auto-detectable with SchemaDetector                   ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ  üéØ USE CASE FEASIBILITY                                                     ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                      ‚îÇ\n",
    "‚îÇ  1. Knowledge-based Q&A (RAG):     ‚úÖ HIGHLY FEASIBLE                        ‚îÇ\n",
    "‚îÇ     - Use existing embedding pipeline                                        ‚îÇ\n",
    "‚îÇ     - Store in MongoDB with vector index                                     ‚îÇ\n",
    "‚îÇ     - Query via BillingRag agents                                            ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ  2. Trend Prediction (ARIMA):      ‚úÖ FEASIBLE                               ‚îÇ\n",
    "‚îÇ     - Need 30+ data points for reliable trends                               ‚îÇ\n",
    "‚îÇ     - Monthly aggregates work well                                           ‚îÇ\n",
    "‚îÇ     - Can predict 1-3 months ahead                                           ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ  3. Anomaly Detection:             ‚úÖ HIGHLY FEASIBLE                        ‚îÇ\n",
    "‚îÇ     - Isolation Forest works with 100+ samples                               ‚îÇ\n",
    "‚îÇ     - Z-Score for simple outliers                                            ‚îÇ\n",
    "‚îÇ     - Can flag unusual billing patterns                                      ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ  üöÄ DEPLOYMENT RECOMMENDATION                                                ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                ‚îÇ\n",
    "‚îÇ  PRIMARY: Your existing Kubernetes cluster                                   ‚îÇ\n",
    "‚îÇ  - Full control, no additional cost                                          ‚îÇ\n",
    "‚îÇ  - Integrate with existing BillingRag deployment                             ‚îÇ\n",
    "‚îÇ  - Use MongoDB GridFS for model storage                                      ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ  ‚ùå NOT RECOMMENDED: Replit                                                  ‚îÇ\n",
    "‚îÇ  - Insufficient RAM for ML training (512MB-2GB)                              ‚îÇ\n",
    "‚îÇ  - Cold starts hurt UX                                                       ‚îÇ\n",
    "‚îÇ  - No enterprise compliance                                                  ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ  üíæ MODEL STORAGE RECOMMENDATION                                             ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                             ‚îÇ\n",
    "‚îÇ  PRIMARY: MongoDB GridFS (already using MongoDB)                             ‚îÇ\n",
    "‚îÇ  BACKUP: Azure Blob Storage (for large Prophet models)                       ‚îÇ\n",
    "‚îÇ  FORMAT: Joblib with compression (best for sklearn)                          ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ  üìÖ MONTHLY RETRAINING STRATEGY                                              ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                               ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Trigger: First Monday of each month                                       ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Process: Ingest ‚Üí Preprocess ‚Üí Train ‚Üí Evaluate ‚Üí Promote/Rollback        ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Versioning: Keep last 3 versions for rollback                             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Storage: ~50KB per model version                                          ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ  ‚è±Ô∏è IMPLEMENTATION TIMELINE (from STATISTICAL_MODELS_IMPLEMENTATION_PLAN)    ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ\n",
    "‚îÇ  Week 1-2: Schema Detection + Data Extractor                                 ‚îÇ\n",
    "‚îÇ  Week 2-4: Time-Series + Anomaly + Trend Modules                             ‚îÇ\n",
    "‚îÇ  Week 4-5: LangChain Integration                                             ‚îÇ\n",
    "‚îÇ  Week 5-6: Testing                                                           ‚îÇ\n",
    "‚îÇ  Week 6-8: Deployment                                                        ‚îÇ\n",
    "‚îÇ  TOTAL: 6-8 weeks                                                            ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "ANSWER TO YOUR QUESTIONS:\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "Q: Is it feasible to train models on monthly billing data?\n",
    "A: ‚úÖ YES - The STATISTICAL_MODELS_IMPLEMENTATION_PLAN.md covers exactly this use case.\n",
    "   Your Excel data has sufficient structure for trend/anomaly detection.\n",
    "\n",
    "Q: Can we use Replit?\n",
    "A: ‚ùå NOT RECOMMENDED for production ML due to:\n",
    "   - Memory limits (512MB-2GB vs 4GB+ needed for Prophet)\n",
    "   - Cold starts (20-60 second delays)\n",
    "   - No enterprise compliance\n",
    "   ‚úÖ USE FOR: Quick demos, prototyping, documentation\n",
    "\n",
    "Q: Where will ML model storage be?\n",
    "A: üéØ RECOMMENDED: MongoDB GridFS\n",
    "   - You already have MongoDB Atlas\n",
    "   - Store models as binary blobs\n",
    "   - Automatic replication\n",
    "   - Easy versioning via metadata collection\n",
    "   \n",
    "   Alternative: Azure Blob Storage for larger models (Prophet ~50MB)\n",
    "\n",
    "NEXT STEPS:\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "1. Run this notebook on your actual billing data\n",
    "2. Implement Phase 1 (Schema Detection) from the plan\n",
    "3. Set up MongoDB GridFS for model storage\n",
    "4. Create K8s CronJob for monthly retraining\n",
    "\"\"\"\n",
    "\n",
    "print(assessment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
