# ğŸ—ï¸ IntelliQuery AI - Complete Architecture Guide

**Version**: 2.1.0  
**Last Updated**: 2026-02-02  
**Status**: Production-Ready

---

## ğŸ“‹ Table of Contents

1. [System Overview](#system-overview)
2. [Architecture Diagram](#architecture-diagram)
3. [Core Components](#core-components)
4. [Agentic Architecture](#agentic-architecture)
5. [Data Flows](#data-flows)
6. [ML Architecture](#ml-architecture)
7. [Enterprise Features](#enterprise-features)
8. [Performance & Scalability](#performance--scalability)
9. [Security Architecture](#security-architecture)
10. [Deployment Architecture](#deployment-architecture)

---

## 1. System Overview

### What is IntelliQuery AI?

IntelliQuery AI is a **production-ready, enterprise-grade Intelligent Customer Analytics Platform** that combines:

- **RAG (Retrieval Augmented Generation)**: Semantic document search with AI-powered answers
- **Natural Language to SQL**: Convert questions to database queries
- **ML Predictions**: Dataset-agnostic classification (churn, attrition, etc.)
- **Autonomous Agents**: Goal-driven multi-step reasoning
- **Interactive Visualizations**: Automatic chart generation

### Key Capabilities

| Capability | Description | Status |
|------------|-------------|--------|
| **Document Q&A** | Upload PDFs/TXT, ask questions, get AI answers | âœ… Production |
| **Data Analytics** | Natural language queries over structured data | âœ… Production |
| **ML Predictions** | Train models, predict outcomes, analyze features | âœ… Production |
| **Agentic Planning** | Multi-step autonomous task execution | âœ… Production |
| **Visualizations** | Auto-generated charts and insights | âœ… Production |

### Technology Stack

```
Frontend:     HTML, JavaScript, Jinja2 Templates
Backend:      Python 3.8+, FastAPI, Uvicorn
ML:           scikit-learn, pandas, numpy
Data:         Databricks (Delta Lake, SQL, Vector Search)
AI:           Databricks LLM & Embedding Endpoints
Caching:      Redis (optional, for distributed rate limiting)
Monitoring:   Structured logging, Health checks
```

---

## 2. Architecture Diagram

### High-Level System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLIENT LAYER                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Web UI      â”‚  â”‚  REST API    â”‚  â”‚  CLI Tools   â”‚          â”‚
â”‚  â”‚  (Browser)   â”‚  â”‚  Clients     â”‚  â”‚              â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MIDDLEWARE LAYER                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Request Context â”‚ Rate Limiting â”‚ Audit Logging          â”‚  â”‚
â”‚  â”‚  Authentication  â”‚ Input Valid.  â”‚ Error Handling         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APPLICATION LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    FastAPI Application                    â”‚   â”‚
â”‚  â”‚  /upload-document  /upload-data  /ask-intelligent        â”‚   â”‚
â”‚  â”‚  /ask-agentic      /train-model  /predict-churn          â”‚   â”‚
â”‚  â”‚  /health           /metrics      /agent/*                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AGENT LAYER  â”‚    â”‚  ANALYTICS    â”‚    â”‚  ML LAYER     â”‚
â”‚               â”‚    â”‚  LAYER        â”‚    â”‚               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚Planner  â”‚ â”‚    â”‚  â”‚Text-SQL â”‚  â”‚    â”‚  â”‚Predictorâ”‚  â”‚
â”‚  â”‚Executor â”‚ â”‚    â”‚  â”‚Router   â”‚  â”‚    â”‚  â”‚Training â”‚  â”‚
â”‚  â”‚Synth.   â”‚ â”‚    â”‚  â”‚Handler  â”‚  â”‚    â”‚  â”‚Inferenceâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                    â”‚                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG LAYER                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Document Processor â”‚ Vector Search â”‚ Embedding Service  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA LAYER                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Databricks â”‚  â”‚ Delta Lake â”‚  â”‚ Vector     â”‚  â”‚ Model    â”‚  â”‚
â”‚  â”‚ SQL        â”‚  â”‚ Tables     â”‚  â”‚ Search     â”‚  â”‚ Registry â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Core Components

### 3.1 API Layer (`src/intelliquery/api/`)

**Purpose**: REST API endpoints and request handling

**Key Files**:
- `app.py` (787 lines): Main FastAPI application with 20+ endpoints

**Endpoints**:
```python
# Document Management
POST   /upload-document          # Upload PDF/TXT for RAG
GET    /documents                # List uploaded documents

# Data Management  
POST   /upload-churn             # Upload CSV/Excel data
GET    /data-schema              # Get table schema
GET    /data-stats               # Get data statistics

# Query & Analysis
POST   /ask-intelligent          # Natural language query (router-based)
POST   /ask-agentic              # Goal-driven query (agent-based)

# ML Operations
GET    /train-model              # Train classification model
POST   /predict-churn            # Single prediction
GET    /predict-batch            # Batch predictions
GET    /feature-importance       # Get feature rankings

# Visualizations
GET    /chart-distribution       # Churn distribution chart
GET    /chart-by-category        # Category breakdown chart
GET    /chart-feature-importance # Feature importance chart

# Agent Operations
POST   /agent/plan               # Create execution plan
POST   /agent/execute            # Execute plan
GET    /agent/tools              # List available tools

# Health & Monitoring
GET    /health                   # Overall health check
GET    /health/live              # Liveness probe (K8s)
GET    /health/ready             # Readiness probe (K8s)
```

**Enterprise Middleware**:
- Request context injection (request ID, timing)
- Rate limiting (in-memory + Redis support)
- Audit logging (all requests logged)
- Error handling (custom exception hierarchy)
- Input validation (file size, type, content)

---

### 3.2 Agent Layer (`src/intelliquery/agent/`)

**Purpose**: Autonomous goal-driven task execution

**Components**:

#### Planner Agent (`planner.py`, 411 lines)
- Interprets user goals
- Decomposes into executable steps
- Selects appropriate tools
- Creates ordered execution plans
- LLM-powered with fallback heuristics

#### Executor (`executor.py`, 230 lines)
- Executes plans step-by-step
- Manages agent state
- Handles dependencies
- Error recovery
- Progress tracking

#### Synthesizer (`synthesizer.py`, 293 lines)
- Transforms results into insights
- Generates natural language summaries
- Provides evidence-backed recommendations
- Confidence scoring

#### Tool Registry (`tools.py`, 323 lines)
- 15+ registered tools
- Standardized tool interface
- Error handling decorator
- Tool categories: RAG, SQL, ML, VISUALIZATION, UTILITY

**Available Tools**:
```python
# RAG Tools
rag_search          # Search documents
rag_answer          # Answer questions using RAG

# SQL Tools
sql_query           # Execute natural language queries
sql_schema          # Get table schema
data_stats          # Get data statistics

# ML Tools
ml_train            # Train classification model
ml_predict          # Single prediction
ml_batch_predict    # Batch predictions
ml_feature_importance # Get feature importance

# Visualization Tools
chart_distribution  # Churn distribution chart
chart_by_category   # Category breakdown chart
chart_feature_importance # Feature importance chart

# Utility Tools
get_current_time    # Get current time
calculate           # Perform calculations
```

---

### 3.3 Analytics Layer (`src/intelliquery/analytics/`)

**Purpose**: Data processing and natural language to SQL

#### Data Handler (`data_handler.py`, 455 lines)
- **Fully dynamic CSV/Excel processing**
- Auto-detects column types
- Creates tables dynamically
- Handles SQL reserved words
- Zero hardcoding

**Key Features**:
```python
# Automatic type inference
INTEGER     # Numeric columns (>80% numeric)
FLOAT       # Decimal numbers
BOOLEAN     # True/False, Yes/No, 0/1
TIMESTAMP   # Date/time columns
STRING      # Everything else

# SQL reserved word handling
"order" â†’ "col_order"
"select" â†’ "col_select"
```

#### Query Router (`query_router.py`, 373 lines)
- Intelligent query classification
- Routes to RAG or SQL
- Confidence scoring
- Keyword-based with patterns

**Query Types**:
```python
KNOWLEDGE   # Document-based (RAG)
DATA        # SQL-based (analytics)
HYBRID      # Both RAG and SQL
```

#### Text-to-SQL (`text_to_sql.py`, 468 lines)
- Dynamic schema detection
- Natural language parsing
- SQL query generation
- Column alias mapping

**Supported Patterns**:
```sql
-- Aggregations
COUNT, AVG, SUM, MIN, MAX

-- Filters
WHERE column = value
WHERE column > value

-- Grouping
GROUP BY column

-- Sorting
ORDER BY column DESC
LIMIT n
```

---

### 3.4 ML Layer (`src/intelliquery/ml/`)

**Purpose**: Machine learning predictions

#### ML Predictor (`predictor.py`, 532 lines)
- **Dataset-agnostic design**
- Auto-detects target column
- Auto-classifies features
- Random Forest classifier
- Model persistence (joblib)

**Algorithm**: Random Forest
```python
RandomForestClassifier(
    n_estimators=100,        # 100 trees
    max_depth=8,             # Balanced depth
    min_samples_split=10,    # Prevent overfitting
    min_samples_leaf=5,      # Leaf size constraint
    max_features='sqrt',     # Feature randomness
    max_samples=0.8,         # Bootstrap 80%
    random_state=42,         # Reproducibility
    n_jobs=-1                # Parallel processing
)
```

**Performance Targets**:
- Accuracy: 75-85%
- Precision: 70-80%
- Recall: 60-75%
- F1 Score: 65-78%
- AUC-ROC: 0.80-0.90

**Key Features**:
- Auto-detects target column (churn, attrition, cancelled, etc.)
- Excludes ID columns automatically
- Handles missing features gracefully
- Provides feature importance rankings
- Risk level classification (HIGH, MEDIUM, LOW)

---

### 3.5 RAG Layer (`src/intelliquery/rag/`)

**Purpose**: Document processing and semantic search

#### Document Processor (`document_processor.py`, 481 lines)
- Text chunking (512 chars, 50-word overlap)
- Batch embedding generation
- Vector storage in Databricks
- Cosine similarity search
- LLM-powered answer generation

**Chunking Strategies**:
```python
# Strategy 1: Fixed-size with overlap
chunk_size = 512 characters
overlap = 50 words

# Strategy 2: Sentence-based
Split on sentence boundaries
Maintain context
```

#### Vector Search (`vector_search.py`, 411 lines)
- Databricks Vector Search integration
- Automatic index creation
- CONTINUOUS and TRIGGERED sync modes
- Change Data Feed enablement
- Fallback to in-memory search

**Performance**:
- Vector Search: O(log n) - scales to millions
- Fallback: O(n) - works up to ~1000 documents

---

### 3.6 Core Layer (`src/intelliquery/core/`)

**Purpose**: Configuration, database, security, monitoring

#### Configuration (`config.py`, 72 lines)
- Environment variable management
- Dynamic table names
- Endpoint configuration
- Validation

#### Database (`database.py`, 198 lines)
- Databricks SQL connection
- Embedding endpoint integration
- LLM endpoint integration
- Mock mode for testing

#### Database Pooled (`database_pooled.py`)
- Thread-safe connection pooling
- Automatic retry with exponential backoff
- Connection health checks
- Pool statistics

#### Security (`security.py`, 316 lines)
- Input validation
- SQL identifier sanitization
- File type validation
- SQL injection prevention

#### Middleware (`middleware.py`, 557 lines)
- Request context injection
- Rate limiting (in-memory + Redis)
- Audit logging
- Circuit breakers
- Timeout control

#### Health (`health.py`, 387 lines)
- Component health checks
- Kubernetes-ready probes
- System metrics (CPU, memory, disk)
- Degraded state detection

#### Error Handler (`error_handler.py`)
- Custom exception hierarchy
- Safe error responses
- Error categorization
- Logging integration

---

## 4. Agentic Architecture

### Agent Execution Flow

```
USER GOAL: "Analyze churn, train a model, and show key factors"
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PLANNER AGENT                             â”‚
â”‚  1. Parse goal into sub-tasks                               â”‚
â”‚  2. Select appropriate tools from registry                   â”‚
â”‚  3. Create ordered execution plan                            â”‚
â”‚  4. Identify dependencies between steps                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚     EXECUTION PLAN       â”‚
             â”‚  Step 1: data_stats      â”‚
             â”‚  Step 2: ml_train        â”‚
             â”‚  Step 3: ml_feature_imp  â”‚
             â”‚  Step 4: chart_features  â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXECUTOR                                  â”‚
â”‚  For each step:                                              â”‚
â”‚    1. Check dependencies met                                 â”‚
â”‚    2. Execute tool from registry                             â”‚
â”‚    3. Update AgentState with results                         â”‚
â”‚    4. Handle errors gracefully                               â”‚
â”‚    5. Continue or abort based on outcome                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENT STATE                               â”‚
â”‚  - goal: Original user goal                                  â”‚
â”‚  - plan: Execution plan                                      â”‚
â”‚  - current_step: Progress tracker                            â”‚
â”‚  - sql_results: Query outputs                                â”‚
â”‚  - model_metrics: Training results                           â”‚
â”‚  - charts: Generated visualizations                          â”‚
â”‚  - insights: Accumulated findings                            â”‚
â”‚  - errors: Error log                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SYNTHESIZER                               â”‚
â”‚  1. Analyze execution results                                â”‚
â”‚  2. Generate natural language summary                        â”‚
â”‚  3. Provide evidence-backed insights                         â”‚
â”‚  4. Create actionable recommendations                        â”‚
â”‚  5. Calculate confidence scores                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
              RESPONSE: Summary + Charts + Recommendations
```

### Agent State Schema

```python
@dataclass
class AgentState:
    goal: str                          # Original user goal
    plan: ExecutionPlan                # Generated plan
    current_step: int                  # Progress tracker
    documents: Optional[List[Dict]]    # RAG results
    sql_results: Optional[Dict]        # Query results
    model_metrics: Optional[Dict]      # ML metrics
    predictions: Optional[List[Dict]]  # Predictions
    charts: List[str]                  # Base64 images
    insights: List[str]                # Accumulated insights
    errors: List[str]                  # Error log
    completed: bool                    # Completion flag
    execution_time: float              # Total time
```

---

## 5. Data Flows

### Flow 1: Document Upload & RAG Query

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. DOCUMENT UPLOAD                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                  User uploads PDF/TXT
                           â”‚
                           â–¼
              [app.py] POST /upload-document
                           â”‚
                           â–¼
          Extract text (pypdf for PDF, decode for TXT)
                           â”‚
                           â–¼
      [document_processor.py] process_document()
                           â”‚
                           â–¼
      Chunk text (512 chars, 50-word overlap)
                           â”‚
                           â–¼
                  For each chunk:
                  â”œâ”€â†’ Get embedding (384-dim vector)
                  â””â”€â†’ Store in rag_documents table
                           â”‚
                           â–¼
              Return: {success, chunks_saved}

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. RAG QUERY                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                  User asks question
                           â”‚
                           â–¼
              [app.py] POST /ask-intelligent
                           â”‚
                           â–¼
              [query_router] route_query()
                           â”‚
                           â–¼
          Classify as KNOWLEDGE (document-based)
                           â”‚
                           â–¼
      [document_processor] answer_question()
                           â”‚
                           â–¼
              search_documents():
              â”œâ”€â†’ Get question embedding
              â”œâ”€â†’ Vector similarity search
              â”œâ”€â†’ Return top-5 documents
              â””â”€â†’ Build context
                           â”‚
                           â–¼
              [database] generate_answer()
              â””â”€â†’ LLM call with context + question
                           â”‚
                           â–¼
          Return: {answer, sources, similarity_scores}
```

**Performance**: 3-8 seconds per query  
**Scalability**: Vector Search scales to millions, fallback to ~1000 docs

---

### Flow 2: Data Upload & Text-to-SQL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. DATA UPLOAD                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                User uploads CSV/Excel
                           â”‚
                           â–¼
              [app.py] POST /upload-churn
                           â”‚
                           â–¼
          [data_handler] process_data_file()
                           â”‚
                           â–¼
              Read file with pandas
                           â”‚
                           â–¼
              Normalize column names:
              â”œâ”€â†’ Lowercase
              â”œâ”€â†’ Spaces â†’ underscores
              â””â”€â†’ Handle SQL reserved words
                           â”‚
                           â–¼
              Infer SQL types:
              â”œâ”€â†’ INTEGER (numeric)
              â”œâ”€â†’ FLOAT (decimal)
              â”œâ”€â†’ BOOLEAN (binary)
              â”œâ”€â†’ TIMESTAMP (dates)
              â””â”€â†’ STRING (text)
                           â”‚
                           â–¼
              CREATE TABLE (if not exists)
                           â”‚
                           â–¼
              Batch INSERT (500 rows/batch)
                           â”‚
                           â–¼
          Return: {success, records_inserted, columns}

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. TEXT-TO-SQL QUERY                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
              User asks data question
                           â”‚
                           â–¼
              [app.py] POST /ask-intelligent
                           â”‚
                           â–¼
              [query_router] route_query()
                           â”‚
                           â–¼
              Classify as DATA (SQL-based)
                           â”‚
                           â–¼
              [text_to_sql] execute_query()
                           â”‚
                           â–¼
              Refresh schema cache
                           â”‚
                           â–¼
              parse_question():
              â”œâ”€â†’ Detect aggregation (COUNT, AVG, etc.)
              â”œâ”€â†’ Extract WHERE conditions
              â”œâ”€â†’ Identify GROUP BY column
              â””â”€â†’ Determine ORDER BY and LIMIT
                           â”‚
                           â–¼
              build_sql():
              â””â”€â†’ Construct valid SQL
                           â”‚
                           â–¼
              [database] query() â†’ Execute SQL
                           â”‚
                           â–¼
              Format results as JSON
                           â”‚
                           â–¼
              Generate natural language answer
                           â”‚
                           â–¼
          Return: {sql, results, answer, row_count}
```

**Performance**: 1-3 seconds per query  
**Supported**: Aggregations, filters, grouping, ordering  
**Not Supported**: JOINs, subqueries, complex WHERE

---

### Flow 3: ML Training & Prediction

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. MODEL TRAINING                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
              User clicks "Train Model"
                           â”‚
                           â–¼
              [app.py] GET /train-model
                           â”‚
                           â–¼
              [predictor] train()
                           â”‚
                           â–¼
              Load data from table (up to 10K rows)
                           â”‚
                           â–¼
              Auto-detect features:
              â”œâ”€â†’ Find target column (churn/attrition/etc.)
              â”œâ”€â†’ Exclude ID and metadata columns
              â”œâ”€â†’ Classify as categorical or numeric
              â””â”€â†’ Encode categorical columns
                           â”‚
                           â–¼
              Split 80/20 train/test (stratified)
                           â”‚
                           â–¼
              Train RandomForest:
              â”œâ”€â†’ 100 trees
              â”œâ”€â†’ max_depth=8
              â”œâ”€â†’ Balanced regularization
              â””â”€â†’ Parallel processing
                           â”‚
                           â–¼
              Evaluate on test set:
              â”œâ”€â†’ Accuracy, Precision, Recall
              â”œâ”€â†’ F1 Score, AUC-ROC
              â””â”€â†’ Confusion matrix
                           â”‚
                           â–¼
              Extract feature importance
                           â”‚
                           â–¼
              Save model to disk (joblib)
                           â”‚
                           â–¼
          Return: {accuracy, metrics, feature_importance}

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. PREDICTION                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
              User submits customer data
                           â”‚
                           â–¼
              [app.py] POST /predict-churn
                           â”‚
                           â–¼
              [predictor] predict()
                           â”‚
                           â–¼
              IF model not trained:
              â””â”€â†’ Auto-train model first
                           â”‚
                           â–¼
              Prepare features (encode categoricals)
                           â”‚
                           â–¼
              Build feature vector
                           â”‚
                           â–¼
              model.predict_proba() â†’ probability
                           â”‚
                           â–¼
              Calculate risk level:
              â”œâ”€â†’ HIGH: prob â‰¥ 0.7
              â”œâ”€â†’ MEDIUM: 0.4 â‰¤ prob < 0.7
              â””â”€â†’ LOW: prob < 0.4
                           â”‚
                           â–¼
              Generate recommendation
                           â”‚
                           â–¼
          Return: {will_churn, probability, risk_level, recommendation}
```

**Performance**: 10-30s training, <100ms prediction  
**Accuracy**: 75-85% (realistic, not overfit)

---

## 6. ML Architecture

### Random Forest Explained

```
Random Forest = Ensemble of Decision Trees

Tree 1:     Tree 2:     Tree 3:     ...     Tree 100:
  â”Œâ”€â”         â”Œâ”€â”         â”Œâ”€â”                 â”Œâ”€â”
  â”‚?â”‚         â”‚?â”‚         â”‚?â”‚                 â”‚?â”‚
  â””â”¬â”˜         â””â”¬â”˜         â””â”¬â”˜                 â””â”¬â”˜
 â”Œâ”€â”´â”€â”       â”Œâ”€â”´â”€â”       â”Œâ”€â”´â”€â”               â”Œâ”€â”´â”€â”
 â”‚   â”‚       â”‚   â”‚       â”‚   â”‚               â”‚   â”‚
Churn No    No Churn   Churn No            No Churn

Final Prediction = Majority Vote
If 60 trees say "Churn" â†’ 60% probability
```

### Hyperparameters

```python
n_estimators=100        # Number of trees
max_depth=8             # Maximum tree depth (prevents overfitting)
min_samples_split=10    # Minimum samples to split node
min_samples_leaf=5      # Minimum samples in leaf
max_features='sqrt'     # Features per split (randomness)
max_samples=0.8         # Bootstrap 80% of data per tree
random_state=42         # Reproducibility
n_jobs=-1               # Use all CPU cores
```

### Why These Values?

- **max_depth=8**: Prevents overfitting (trees not too deep)
- **min_samples_split=10**: Requires enough data to split
- **min_samples_leaf=5**: Prevents tiny leaves (overfitting)
- **max_features='sqrt'**: Adds randomness (better generalization)
- **max_samples=0.8**: Each tree sees different data (diversity)

### Feature Importance

Random Forest calculates importance by measuring how much each feature reduces impurity (Gini index) across all trees.

**Example Output**:
```python
{
    "Contract_encoded": 0.25,      # 25% importance
    "Tenure Months": 0.20,         # 20% importance
    "Monthly Charges": 0.15,       # 15% importance
    "Internet Service_encoded": 0.12,
    "Tech Support_encoded": 0.10,
    ...
}
```

---

## 7. Enterprise Features

### 7.1 Security

#### Input Validation
```python
# File upload validation
MAX_FILE_SIZE = 50MB
ALLOWED_TYPES = [PDF, TXT, CSV, XLSX]

# Query validation
MAX_QUERY_LENGTH = 10,000 chars
SQL_INJECTION_PATTERNS = blocked

# Identifier sanitization
SQL_RESERVED_WORDS = escaped
```

#### SQL Injection Prevention
```python
# Parameterized queries
# Identifier allowlisting
# Input sanitization
# Pattern blocking
```

#### Authentication/Authorization (Framework Ready)
```python
# OAuth2/OIDC stubs
# RBAC framework
# Token validation
# Role-based access control

# Needs configuration:
- OAuth2 provider (Azure AD, Okta, Auth0)
- Role definitions
- Permission mappings
```

---

### 7.2 Rate Limiting

#### In-Memory (Single Instance)
```python
RateLimitConfig:
    requests_per_minute: 60
    requests_per_hour: 1000
    burst_limit: 10
```

#### Redis (Distributed)
```python
# Sliding window algorithm
# Atomic operations
# Multi-instance support
# Automatic cleanup
```

---

### 7.3 Error Handling

#### Exception Hierarchy
```python
IntelliQueryError
â”œâ”€â”€ ValidationError
â”œâ”€â”€ DatabaseError
â”œâ”€â”€ ModelError
â”œâ”€â”€ AuthenticationError
â””â”€â”€ TimeoutError
```

#### Safe Error Responses
```python
# Never expose internal details
# Log full errors server-side
# Return user-friendly messages
# Include request ID for support
```

---

### 7.4 Health Checks

#### Kubernetes-Ready Probes
```python
GET /health          # Overall health
GET /health/live     # Liveness probe
GET /health/ready    # Readiness probe
```

#### Component Checks
```python
- Database connectivity
- Embedding service
- LLM service
- Disk space
- Memory usage
- ML model status
```

---

### 7.5 Audit Logging

```python
# All requests logged with:
- timestamp
- request_id
- user_id (when auth enabled)
- method, path
- client_ip
- status_code
- duration_ms
- query_params
```

---

### 7.6 Circuit Breakers

```python
# For external services:
llm_circuit_breaker
embedding_circuit_breaker
database_circuit_breaker

# States: CLOSED â†’ OPEN â†’ HALF_OPEN
# Automatic recovery
# Fail-fast behavior
```

---

## 8. Performance & Scalability

### Performance Metrics

| Operation | Current | Target | Status |
|-----------|---------|--------|--------|
| Document upload | 2-5s | <2s | âš ï¸ |
| Question answering | 3-8s | <3s | âš ï¸ |
| Data upload (5K rows) | 5-15s | <10s | âœ… |
| SQL query | 1-3s | <2s | âœ… |
| Model training | 10-30s | <20s | âš ï¸ |
| Single prediction | <100ms | <100ms | âœ… |

### Scalability Considerations

#### Vector Search
- **Current**: O(n) fallback for <1000 docs
- **Production**: Databricks Vector Search (O(log n))
- **Scales to**: Millions of documents

#### Connection Pooling
- **Current**: Single connection (development)
- **Production**: Connection pool (5-20 connections)
- **Implemented**: `database_pooled.py` (not enabled)

#### Rate Limiting
- **Current**: In-memory (single instance)
- **Production**: Redis-backed (distributed)
- **Implemented**: Both options available

#### Caching
- **Current**: Schema caching only
- **Recommended**: Redis for query results, embeddings
- **Impact**: 50-80% latency reduction

---

## 9. Security Architecture

### Defense in Depth

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: Network Security                                    â”‚
â”‚  - Load balancer with DDoS protection                        â”‚
â”‚  - SSL/TLS termination                                       â”‚
â”‚  - IP allowlisting (optional)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 2: API Gateway                                         â”‚
â”‚  - Authentication (OAuth2/OIDC)                              â”‚
â”‚  - Authorization (RBAC)                                      â”‚
â”‚  - Rate limiting                                             â”‚
â”‚  - Request validation                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 3: Application Security                                â”‚
â”‚  - Input validation                                          â”‚
â”‚  - SQL injection prevention                                  â”‚
â”‚  - XSS protection                                            â”‚
â”‚  - CSRF protection                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 4: Data Security                                       â”‚
â”‚  - Encryption at rest (Databricks)                           â”‚
â”‚  - Encryption in transit (TLS)                               â”‚
â”‚  - Access control (table-level)                              â”‚
â”‚  - Audit logging                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10. Deployment Architecture

### Development
```
Single Instance
â”œâ”€â”€ In-memory rate limiting
â”œâ”€â”€ Single database connection
â”œâ”€â”€ Local file storage
â””â”€â”€ Mock mode for testing
```

### Staging
```
2-3 Instances (Load Balanced)
â”œâ”€â”€ Redis rate limiting
â”œâ”€â”€ Connection pooling (5-10)
â”œâ”€â”€ Databricks Vector Search
â””â”€â”€ Basic monitoring
```

### Production
```
5+ Instances (Auto-scaling)
â”œâ”€â”€ Redis cluster (distributed)
â”œâ”€â”€ Connection pooling (10-20)
â”œâ”€â”€ Databricks Vector Search
â”œâ”€â”€ Prometheus metrics
â”œâ”€â”€ Distributed tracing
â”œâ”€â”€ Alerting (PagerDuty/Slack)
â””â”€â”€ Full audit logging
```

### Kubernetes Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: intelliquery-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: intelliquery-api
  template:
    metadata:
      labels:
        app: intelliquery-api
    spec:
      containers:
      - name: api
        image: intelliquery:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABRICKS_HOST
          valueFrom:
            secretKeyRef:
              name: databricks-secrets
              key: host
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
```

---

## ğŸ“š Additional Resources

- **Issues & Improvements**: See `ISSUES_AND_IMPROVEMENTS.md`
- **Dataset Guide**: See `DATASET_ADAPTABILITY_GUIDE.md`
- **Sample Queries**: See `SAMPLE_QUERIES.md`
- **API Documentation**: Visit `/docs` endpoint

---

## ğŸ¯ Quick Reference

### File Structure
```
src/intelliquery/
â”œâ”€â”€ agent/          # Agentic architecture (planner, executor, synthesizer)
â”œâ”€â”€ analytics/      # Data handling, text-to-SQL, query routing
â”œâ”€â”€ api/            # FastAPI application, endpoints
â”œâ”€â”€ core/           # Config, database, security, middleware, health
â”œâ”€â”€ ml/             # ML predictor, training, inference
â”œâ”€â”€ rag/            # Document processing, vector search
â”œâ”€â”€ utils/          # Utility functions
â””â”€â”€ visualization/  # Chart generation
```

### Key Metrics
- **Lines of Code**: ~8,000
- **Endpoints**: 20+
- **Tools**: 15+
- **Components**: 25+
- **Documentation**: 3,300+ lines

### Version History
- **v2.1.0** (Current): Enterprise features, agentic architecture
- **v2.0.0**: Dataset-agnostic ML, model persistence
- **v1.0.0**: Initial release

---

**Document Version**: 2.1.0  
**Last Updated**: 2026-02-02  
**Status**: Production-Ready

*Built with â¤ï¸ by IBM Bob - AI Software Engineer*